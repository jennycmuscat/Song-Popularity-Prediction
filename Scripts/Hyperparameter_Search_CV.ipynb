{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2023-11-27T11:47:50.115018Z",
     "start_time": "2023-11-27T11:47:39.513065800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from utils import dataframe_to_tensor_dataset, train_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.3.2'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "sklearn.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T11:53:24.615858900Z",
     "start_time": "2023-11-27T11:53:24.608885400Z"
    }
   },
   "id": "3de27dff06732d0b"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the training set\n",
    "train_set_numerical = pd.read_csv(\"./Data/training.csv\")"
   ],
   "metadata": {
    "id": "VMuT4DbpZzhR"
   },
   "id": "VMuT4DbpZzhR",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Method for normalising the training split, used in K-fold validation \n",
    "# Returns the normalised training split, its features' mean and standard deviation\n",
    "def normalise_training_split(dataframe):\n",
    "\n",
    "  dataframe = dataframe.copy()\n",
    "  numerical_features= ['Total Followers', 'Danceability', 'Energy',\n",
    "       'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness',\n",
    "       'Valence']\n",
    "\n",
    "  \n",
    "  means = {}\n",
    "  stds = {}\n",
    "  \n",
    "  # Iterate over all features in a dataframe\n",
    "  for column in dataframe[numerical_features]:\n",
    "\n",
    "      # Calculate the mean and the standard deviation for the feature\n",
    "      mean = dataframe[column].mean()\n",
    "      std = dataframe[column].std()\n",
    "      means[column] = mean\n",
    "      stds[column] = std\n",
    "      \n",
    "      # Normalise the feature\n",
    "      dataframe.loc[:,column] =  (dataframe[column] - mean ) / std\n",
    "\n",
    "  return dataframe, means, stds"
   ],
   "metadata": {
    "id": "7OtyuLsFdMpV"
   },
   "id": "7OtyuLsFdMpV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Method for normalising the validation split, used in K-fold validation \n",
    "# Returns the normalised validation split.\n",
    "def normalise_validation_split(dataframe, means, stds):\n",
    "\n",
    "  dataframe = dataframe.copy()\n",
    "  numerical_features= ['Total Followers', 'Danceability', 'Energy',\n",
    "       'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness',\n",
    "       'Valence']\n",
    "\n",
    "  # Normalising each feature using Z-score normalisation\n",
    "  for column in dataframe[numerical_features]:\n",
    "\n",
    "        mean = means[column]\n",
    "        std = stds[column]\n",
    "        \n",
    "        # Normalise the feature\n",
    "        dataframe.loc[:,column] =  (dataframe[column] - mean ) / std\n",
    "\n",
    "  return dataframe\n"
   ],
   "metadata": {
    "id": "xTvIAfdLyjKI"
   },
   "id": "xTvIAfdLyjKI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Method that performs the hyperparameter grid search for a model, using K-fold cross validation\n",
    "def model_hyperparameter_search(model_dictionary):\n",
    "    \n",
    "    \n",
    "    output_path = model_dictionary[\"output\"]\n",
    "    # Obtain the model feature extraction information\n",
    "    audio_features_id = model_dictionary[\"audio_features_id\"]\n",
    "    num_audio_features = model_dictionary[\"num_audio_features\"]\n",
    "    followers_id = model_dictionary[\"followers_id\"]\n",
    "    \n",
    "    random_state = 10\n",
    "    max_epochs = 150\n",
    "    \n",
    "    # Define the grid search hyperparameter lists\n",
    "    batch_sizes = [2**i for i in range(5,9)]\n",
    "    epochs = [10*i for i in range(1,16)]\n",
    "    epochs = [1]\n",
    "    \n",
    "    hidden_units = [2**i for i in range(5,10)]\n",
    "    hidden_units = [3]\n",
    "    \n",
    "    # Write the obtained results in a file\n",
    "    with open(output_path, 'w', encoding='UTF8', newline='') as f:\n",
    "    \n",
    "        writer = csv.writer(f)\n",
    "        header = [\"batch_size\", \"epochs\", \"hidden_units\", \"train_loss\", \"val_loss\", \"train_mae\", \"val_mae\"]\n",
    "        writer.writerow(header)\n",
    "    \n",
    "        # Iterate over all batch sizes\n",
    "        for batch_size in tqdm(batch_sizes):\n",
    "    \n",
    "            # Iterate over all hidden units\n",
    "            for hidden_unit in tqdm(hidden_units):\n",
    "    \n",
    "                splits = 10\n",
    "                \n",
    "                # Construct training and validation splits for 10 folds\n",
    "                k_fold = KFold(n_splits = splits, shuffle = True, random_state = random_state)\n",
    "    \n",
    "                training_histories = []\n",
    "                \n",
    "                # Iterate over 10 training and validation folds\n",
    "                for k_id, (training_indices, validation_indices) in tqdm(enumerate(k_fold.split(train_set_numerical))):\n",
    "    \n",
    "                    # Construct the training and validation sets\n",
    "                    k_training = train_set_numerical.iloc[training_indices,:]\n",
    "                    k_validation = train_set_numerical.iloc[validation_indices,:]\n",
    "    \n",
    "                    # Normalise the training and validation sets\n",
    "                    k_training_norm, mean, std = normalise_training_split(k_training)\n",
    "                    k_validation_norm = normalise_validation_split(k_validation, mean, std)\n",
    "    \n",
    "                    # Obtain Tensorflow training and validation datasets\n",
    "                    train_dataset = dataframe_to_tensor_dataset(k_training_norm, 11, 7, 7, batch_size, audio_features_id = audio_features_id, num_audio_features=num_audio_features, followers_id = followers_id)\n",
    "                    valid_dataset = dataframe_to_tensor_dataset(k_validation_norm, 11, 7, 7, batch_size, audio_features_id = audio_features_id, num_audio_features=num_audio_features, followers_id = followers_id)\n",
    "    \n",
    "                    # Obtain model training history\n",
    "                    training_history, _ = train_model(hidden_unit,max_epochs,train_dataset,valid_dataset)\n",
    "                    training_histories.append(training_history)\n",
    "    \n",
    "                # Iterate over every 10th training epoch and store the average K-fold metrics in a file\n",
    "                for epoch in epochs:\n",
    "    \n",
    "                    t_loss_total = 0\n",
    "                    v_loss_total = 0\n",
    "                    t_mae_total = 0\n",
    "                    v_mae_total = 0\n",
    "    \n",
    "                    # Calculate the total MSE and MAE for training and validation sets for all K-folds\n",
    "                    for training_history in training_histories:\n",
    "    \n",
    "                      t_loss_total += training_history[\"loss\"][epoch-1]\n",
    "                      v_loss_total += training_history[\"val_loss\"][epoch-1]\n",
    "                      t_mae_total += training_history[\"mean_absolute_error\"][epoch-1]\n",
    "                      v_mae_total += training_history[\"val_mean_absolute_error\"][epoch-1]\n",
    "    \n",
    "                    # Calculate the average MSE and MAE for training and validation sets and store in a file\n",
    "                    metrics = [t_loss_total / splits, v_loss_total  / splits, t_mae_total  / splits, v_mae_total  / splits]\n",
    "                    formatted_metrics = [f'{metric:.3f}' for metric in metrics]\n",
    "                    writer.writerow([batch_size, epoch, hidden_unit] + formatted_metrics)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64c22e97a0583e56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define hyperparameter search dictionaries for all 4 model types\n",
    "model_1 = {\n",
    "    'output': os.path.join(\"Data\",\"Hyperparameter_Search\", \"hyperparameters_points.csv\"),\n",
    "    'audio_features_id': None, \n",
    "    'num_audio_features': None,\n",
    "    'followers_id': None\n",
    "}\n",
    "\n",
    "model_2 = {\n",
    "    'output': os.path.join(\"Data\",\"Hyperparameter_Search\", \"hyperparameters_points_audio.csv\"),\n",
    "    'audio_features_id': 3, \n",
    "    'num_audio_features': 7,\n",
    "    'followers_id': None\n",
    "}\n",
    "\n",
    "model_3 = {\n",
    "    'output': os.path.join(\"Data\",\"Hyperparameter_Search\", \"hyperparameters_points_followers.csv\"),\n",
    "    'audio_features_id': None, \n",
    "    'num_audio_features': None,\n",
    "    'followers_id': 2\n",
    "}\n",
    "\n",
    "model_4 = {\n",
    "    'output': os.path.join(\"Data\",\"Hyperparameter_Search\", \"hyperparameters_points_audio_followers.csv\"),\n",
    "    'audio_features_id': 3, \n",
    "    'num_audio_features': 7,\n",
    "    'followers_id': 2\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c01e40e943b77e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform hyperparameter search for the 4 model types, saving the results in a file\n",
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "    model_hyperparameter_search(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e45c8463aa33eb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
