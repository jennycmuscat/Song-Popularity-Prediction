{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset restructuring, missing value imputation\n",
    "In this notebook the updated Spotify dataset (containing artist followers) is transformed to a dataset containing all the **unique songs** with their respective **audio features, sum of artist followers and points for the first 14 days** since the song's debut in the charts. For songs that contains some missing dates in the dataset, missing value imputation is performed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a76c3df2e933c6f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T13:08:52.377553200Z",
     "start_time": "2023-11-26T13:08:48.273841300Z"
    }
   },
   "id": "7eb409567c65e437"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:17:46.808950Z",
     "start_time": "2023-11-15T16:17:44.519411100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Spotify dataset containing Top 200 playlists\n",
    "path_to_dataset = os.path.join(\"..\", \"Data\",\"spotify_updated_2.csv\")\n",
    "spotify_dataframe = pd.read_csv(path_to_dataset, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Create a new dataframe where each row represents a unique song\n",
    "unique_songs = spotify_dataframe.drop_duplicates([\"Title\", \"Artists\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:22:10.027524800Z",
     "start_time": "2023-11-15T16:22:09.925946800Z"
    }
   },
   "id": "758cf01e88116acb"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Define the dates that aren't present in the dataset\n",
    "missing_dates = pd.to_datetime([\"2017-02-23\", \"2017-06-02\", \"2017-05-30\", \"2017-05-31\"]).values\n",
    "\n",
    "# missing_single_dates = missing_dates[:2]\n",
    "feb_23 = missing_dates[0]\n",
    "june_2 = missing_dates[1]\n",
    "may_30 = missing_dates[2]\n",
    "may_31 = missing_dates[3]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:17:48.295533400Z",
     "start_time": "2023-11-15T16:17:48.273383700Z"
    }
   },
   "id": "86c063e329afa65c"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Function for imputing the points for songs on dates that are missing\n",
    "def impute_points(current_date,song_dataframe):\n",
    "    \n",
    "    missing_dates = pd.to_datetime([\"2017-02-23\", \"2017-06-02\", \"2017-05-30\", \"2017-05-31\"]).values\n",
    "    \n",
    "    \n",
    "    if current_date == missing_dates[0] or current_date == missing_dates[1]:\n",
    "        \n",
    "        previous_date = current_date - datetime.timedelta(days=1)\n",
    "        \n",
    "        # Check if song was present on the chart on the previous day, if so - obtain the points on \n",
    "        # the respective day\n",
    "        if previous_date not in song_dataframe[\"Date\"]:\n",
    "            points_on_previous_day = 0\n",
    "        else:\n",
    "            points_on_previous_day = song_dataframe[song_dataframe[\"Date\"] == previous_date][\"Points (Total)\"].values[0]\n",
    "            \n",
    "        next_date = current_date+ datetime.timedelta(days=1)\n",
    "        \n",
    "        # Check if song was present on the chart on the next day, if so - obtain the points on the \n",
    "        # respective day\n",
    "        if next_date not in song_dataframe[\"Date\"]:\n",
    "            points_on_next_day = 0\n",
    "        else:\n",
    "            points_on_next_day = song_dataframe[song_dataframe[\"Date\"] == next_date][\"Points (Total)\"].values[0]\n",
    "            \n",
    "    \n",
    "    elif current_date == missing_dates[2]:\n",
    "        \n",
    "        previous_date = current_date - datetime.timedelta(days=1)\n",
    "        \n",
    "        # Check if song was present on the chart on the previous day, if so - obtain the points on \n",
    "        # the respective day\n",
    "        if previous_date not in song_dataframe[\"Date\"]:\n",
    "            points_on_previous_day = 0\n",
    "        else:\n",
    "            points_on_previous_day = song_dataframe[song_dataframe[\"Date\"] == previous_date][\"Points (Total)\"].values[0]\n",
    "            \n",
    "        next_date = current_date+ datetime.timedelta(days=2)\n",
    "        \n",
    "        # Check if song was present on the chart on 2 days after, if so - obtain the points on \n",
    "        # the respective day\n",
    "        if next_date not in song_dataframe[\"Date\"]:\n",
    "            points_on_next_day = 0\n",
    "        else:\n",
    "            points_on_next_day = song_dataframe[song_dataframe[\"Date\"] == next_date][\"Points (Total)\"].values[0]\n",
    "     \n",
    "    elif current_date == missing_dates[3]:\n",
    "        \n",
    "        previous_date = current_date - datetime.timedelta(days=2)\n",
    "        \n",
    "        # Check if song was present on the chart 2 days ago, if so - obtain the points on \n",
    "        # the respective day\n",
    "        if previous_date not in song_dataframe[\"Date\"]:\n",
    "            points_on_previous_day = 0\n",
    "        else:\n",
    "            points_on_previous_day = song_dataframe[song_dataframe[\"Date\"] == previous_date][\"Points (Total)\"].values[0]\n",
    "            \n",
    "        next_date = current_date+ datetime.timedelta(days=1)\n",
    "        \n",
    "        # Check if song was present on the chart on the next day if so - obtain the points on \n",
    "        # the respective day\n",
    "        if next_date not in song_dataframe[\"Date\"]:\n",
    "            points_on_next_day = 0\n",
    "        else:\n",
    "            points_on_next_day = song_dataframe[song_dataframe[\"Date\"] == next_date][\"Points (Total)\"].values[0]       \n",
    "        \n",
    "    # Calculate the points on the missing date by taking the average of the previous and next day points\n",
    "    points_on_date = round((points_on_previous_day + points_on_next_day) / 2)\n",
    "        \n",
    "    return points_on_date\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:17:49.509141800Z",
     "start_time": "2023-11-15T16:17:49.504136300Z"
    }
   },
   "id": "3f4ca198f83da2eb"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7801it [09:37, 13.50it/s]\n"
     ]
    }
   ],
   "source": [
    "row_list = []\n",
    "row_index = 0\n",
    "\n",
    "# Iterate over unique songs \n",
    "for _, unique_song in tqdm(unique_songs.iterrows()):\n",
    "    \n",
    "    title = unique_song[\"Title\"]\n",
    "    artists = unique_song[\"Artists\"]\n",
    "    \n",
    "    # Extract all songs with the specified song title and artist\n",
    "    song_dataframe = spotify_dataframe[(spotify_dataframe[\"Title\"] == title) & (spotify_dataframe[\"Artists\"] == artists)]\n",
    "    \n",
    "   \n",
    "    # Obtain the artists of the songs and their respective followers\n",
    "    unique_artists = song_dataframe.drop_duplicates(\"# of Artist\")[[\"# of Artist\", \"Followers\"]]\n",
    "    \n",
    "    # Calculate the total followers all the artists of a song have\n",
    "    total_followers = np.sum(unique_artists.iloc[:,1].astype(int))\n",
    "\n",
    "    # Obtain entries for each unique date\n",
    "    song_dataframe = song_dataframe.drop_duplicates(\"Date\")\n",
    "    \n",
    "    # Convert the date to 'datetime' data type and sort them in ascending order\n",
    "    song_dataframe[\"Date\"] = pd.to_datetime(song_dataframe[\"Date\"], format = \"%d/%m/%Y\")\n",
    "    song_dataframe = song_dataframe.sort_values(\"Date\")\n",
    "    \n",
    "    # Obtain the date when the song first appeared in the charts\n",
    "    first_date = song_dataframe.iloc[0,:][\"Date\"]\n",
    "    \n",
    "    # Obtain the 'datetime' objects for the next 14 days, starting with the song's debut\n",
    "    next_14_dates = [first_date + datetime.timedelta(days=day) for day in range(14)]\n",
    "\n",
    "    points_14_days = []\n",
    "    \n",
    "    # Iterate over the 14 days, since the song's debut\n",
    "    for date in next_14_dates:\n",
    "        \n",
    "        \n",
    "        # If song's debut 14-day window includes one of the missing dates, perform point imputing\n",
    "        if date == may_30 or date == may_31 or date == feb_23 or date == june_2:\n",
    "            points_on_date = impute_points(date, song_dataframe)\n",
    "            \n",
    "        # Check if song was on the chart on the given date\n",
    "        elif (song_dataframe[\"Date\"] == date).any():\n",
    "            # Add the points it received on the given date to the array\n",
    "            points_on_date = song_dataframe[song_dataframe[\"Date\"] == date][\"Points (Total)\"].values[0]\n",
    "\n",
    "        # If the song wasn't on the chart on the given date, set points to 0\n",
    "        else:\n",
    "            points_on_date = 0\n",
    "            \n",
    "        points_14_days.append(points_on_date)\n",
    "        \n",
    "\n",
    "    # Construct the output row for the song, containing information about the song, audio features and points it received \n",
    "    # on the first 14 days since its debut\n",
    "    output_row = [song_dataframe[\"Title\"].iloc[0], song_dataframe[\"Artists\"].iloc[0], total_followers, song_dataframe[\"Danceability\"].iloc[0], song_dataframe[\"Energy\"].iloc[0], song_dataframe[\"Loudness\"].iloc[0], song_dataframe[\"Speechiness\"].iloc[0], song_dataframe[\"Acousticness\"].iloc[0], song_dataframe[\"Instrumentalness\"].iloc[0], song_dataframe[\"Valence\"].iloc[0], first_date] + points_14_days\n",
    "  \n",
    "  \n",
    "    # Add the output row to the song dataframe\n",
    "    row_list.append(output_row)\n",
    "    row_index+=1\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:50.713090400Z",
     "start_time": "2023-11-15T16:22:12.841061Z"
    }
   },
   "id": "a106b167cb71d87f"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Create the column names for the new dataset\n",
    "columns = ['Title', 'Artists', 'Total Followers', 'Danceability', 'Energy',\n",
    "       'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness',\n",
    "       'Valence', 'First Date']\n",
    "\n",
    "for i in range(1,15):\n",
    "    columns.append(f\"Points_{i}\")\n",
    "    \n",
    "song_rank_df = pd.DataFrame(row_list, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:51.096482500Z",
     "start_time": "2023-11-15T16:31:50.720086400Z"
    }
   },
   "id": "68cbe61b647fd607"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Ensure the loudness values are within range [-60;0]\n",
    "\n",
    "# After inspecting the dataset, it was found that the values less than or equal to -1000 were exactly\n",
    "# 1000 times less than their actual values, thus we are dividing such values by 1000 \n",
    "song_rank_df.loc[song_rank_df[\"Loudness\"] <= -1000, \"Loudness\"] = song_rank_df.loc[song_rank_df[\"Loudness\"] <= -1000, \"Loudness\"] / 1000\n",
    "\n",
    "# Drop the songs, where the loudness was greater than 0\n",
    "song_rank_df = song_rank_df[song_rank_df[\"Loudness\"] <= 0]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:51.096482500Z",
     "start_time": "2023-11-15T16:31:51.073242600Z"
    }
   },
   "id": "1cccb82bcf5e4043"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 7798 unique songs before normalisation\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset contains {song_rank_df.shape[0]} unique songs before normalisation\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:51.097483400Z",
     "start_time": "2023-11-15T16:31:51.083271300Z"
    }
   },
   "id": "9fbd5d7ad71bdd12"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Save the new dataset to a file\n",
    "song_rank_df.to_csv(os.path.join(\"..\", \"Data\",\"song_dataset.csv\"), index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:32:11.838996300Z",
     "start_time": "2023-11-15T16:32:11.764650400Z"
    }
   },
   "id": "d0b565b35fd31277"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def normalise_testing_set(dataframe, means, stds):\n",
    "\n",
    "  dataframe = dataframe.copy()\n",
    "  numerical_features= ['Total Followers', 'Danceability', 'Energy',\n",
    "       'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness',\n",
    "       'Valence']\n",
    "\n",
    "  # Normalising each feature using Z-score normalisation\n",
    "  for column in dataframe[numerical_features]:\n",
    "\n",
    "        mean = means[column]\n",
    "        std = stds[column]\n",
    "        # Normalise the feature\n",
    "        dataframe.loc[:,column] =  (dataframe[column] - mean ) / std\n",
    "\n",
    "  return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T13:10:30.466395900Z",
     "start_time": "2023-11-26T13:10:30.450763Z"
    }
   },
   "id": "618cb56caf279800"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def normalise_training_set(dataframe):\n",
    "\n",
    "  dataframe = dataframe.copy()\n",
    "  numerical_features= ['Total Followers', 'Danceability', 'Energy',\n",
    "       'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness',\n",
    "       'Valence']\n",
    "\n",
    "  # Normalising each feature using Z-score normalisation\n",
    "  means = {}\n",
    "  stds = {}\n",
    "  for column in dataframe[numerical_features]:\n",
    "\n",
    "      # Calculate the mean and the standard deviation\n",
    "      mean = dataframe[column].mean()\n",
    "      std = dataframe[column].std()\n",
    "      \n",
    "      means[column] = mean\n",
    "      stds[column] = std\n",
    "      # Normalise the feature\n",
    "      dataframe.loc[:,column] =  (dataframe[column] - mean ) / std\n",
    "\n",
    "  return dataframe, means, stds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T13:10:28.943377600Z",
     "start_time": "2023-11-26T13:10:28.927757800Z"
    }
   },
   "id": "c1fbfb005eea022a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shuffle the dataset rows\n",
    "random_state = 10\n",
    "\n",
    "# Create training (90%) and test (10%) splits\n",
    "shuffled_dataset = song_rank_df.sample(frac=1, random_state=random_state)\n",
    "train, val, test = np.split(shuffled_dataset, [int(len(shuffled_dataset) * 0.9)])\n",
    "\n",
    "train.to_csv(os.path.join(\"..\", \"Data\",\"training.csv\"), index = False)\n",
    "test.to_csv(os.path.join(\"..\", \"Data\",\"testing.csv\"), index = False)\n",
    "\n",
    "# Normalise the training and test splits\n",
    "normalised_training, means, stds = normalise_training_set(train)\n",
    "normalised_testing = normalise_testing_set(test, means, stds)\n",
    "\n",
    "normalised_training.to_csv(os.path.join(\"..\", \"Data\",\"training_normalised.csv\"), index = False)\n",
    "normalised_testing.to_csv(os.path.join(\"..\", \"Data\",\"testing_normalised.csv\"), index = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a352016e4edea5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
