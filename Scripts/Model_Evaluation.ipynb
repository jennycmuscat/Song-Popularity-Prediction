{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import dataframe_to_tensor_dataset, BaselineRepeatLast, LinearBaseline, R2_Score, train_model\n",
    "from tensorflow import keras\n",
    "import scipy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b31175264c4d4e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the normalised training and testing set dataframes\n",
    "train_df = pd.read_csv(os.path.join(\"Data\", \"training_normalised.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(\"Data\", \"testing_normalised.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8447e86856d7399f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the Baseline 1 model on the testing set\n",
    "\n",
    "# Obtain the Tensorflow dataset\n",
    "test_dataset = dataframe_to_tensor_dataset(test_df, 11, 7, 7, 32)\n",
    "\n",
    "# Initialise the baseline model\n",
    "baseline_1 = BaselineRepeatLast()\n",
    "baseline_1.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                      metrics=[ keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Obtain the MSE, MAE results for the Baseline 1 model\n",
    "evaluation_results = baseline_1.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "# Obtain the R2 score for the Baseline 1 model\n",
    "predictions_1 = baseline_1.predict(test_dataset, verbose=0)\n",
    "prediction_vector_1 = predictions_1.squeeze().flatten()\n",
    "prediction_batched_1 = np.expand_dims(prediction_vector_1, axis = 1)\n",
    "\n",
    "targets = np.concatenate([target for _, target in test_dataset], axis=0)\n",
    "targets = targets.squeeze().flatten()\n",
    "targets_batched = np.expand_dims(targets, axis = 1)\n",
    "\n",
    "mse = evaluation_results[0]\n",
    "mae = evaluation_results[1]\n",
    "r2 = R2_Score(prediction_batched_1, targets_batched)\n",
    "\n",
    "print(f\"When evaluated on testing set, the Baseline 1 model achieves MSE of {mse:.3f}, MAE of {mae:.3f} and R2 score of {r2:.3f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "404a0de9742deb9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the Baseline 2 model on the testing set\n",
    "\n",
    "# Initialise the baseline model\n",
    "baseline_2 = LinearBaseline()\n",
    "baseline_2.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                      metrics=[ keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Obtain the MSE, MAE results for the Baseline 2 model\n",
    "evaluation_results = baseline_2.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "# Obtain the R2 score for the Baseline 2 model\n",
    "predictions_2 = baseline_2.predict(test_dataset, verbose=0)\n",
    "prediction_vector_2 = predictions_2.squeeze().flatten()\n",
    "prediction_batched_2 = np.expand_dims(prediction_vector_2, axis = 1)\n",
    "\n",
    "mse = evaluation_results[0]\n",
    "mae = evaluation_results[1]\n",
    "r2 = R2_Score(prediction_batched_2, targets_batched)\n",
    "\n",
    "print(f\"When evaluated on testing set, the Baseline 2 model achieves MSE of {mse:.3f}, MAE of {mae:.3f} and R2 score of {r2:.3f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67bf1d9e437f0ecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation of 4 LSTM models\n",
    "\n",
    "# Define the hyperparameters for the 4 best models:\n",
    "model_1 = {\n",
    "    'hidden_units': 32,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 130,\n",
    "    'audio_features_id': None, \n",
    "    'num_audio_features': None,\n",
    "    'followers_id': None}\n",
    "\n",
    "model_2 = {\n",
    "    'hidden_units': 128,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 40,\n",
    "    'audio_features_id': 3, \n",
    "    'num_audio_features': 7,\n",
    "    \n",
    "    'followers_id': None}\n",
    "\n",
    "model_3 = {\n",
    "    'hidden_units': 32,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 150,\n",
    "    'audio_features_id': None, \n",
    "    'num_audio_features': None,\n",
    "    'followers_id': 2}\n",
    "\n",
    "model_4 = {\n",
    "    'hidden_units': 256,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'audio_features_id': 3, \n",
    "    'num_audio_features': 7,\n",
    "    'followers_id': 2}\n",
    "\n",
    "\n",
    "# Iterate over all 4 models. Calculate their MSE, MAE and R2 scores on the test set\n",
    "for id, model in enumerate([model_1, model_2, model_3, model_4]):\n",
    "    \n",
    "    batch_size = model[\"batch_size\"]\n",
    "    hidden_units = model[\"hidden_units\"]\n",
    "    epochs = model[\"epochs\"]\n",
    "    \n",
    "    audio_features_id = model[\"audio_features_id\"]\n",
    "    num_audio_features = model[\"num_audio_features\"]\n",
    "    followers_id = model[\"followers_id\"]\n",
    "    \n",
    "    \n",
    "    train_dataset = dataframe_to_tensor_dataset(train_df, 11, 7, 7, batch_size, audio_features_id = audio_features_id, num_audio_features=num_audio_features, followers_id= followers_id)\n",
    "    test_dataset = dataframe_to_tensor_dataset(test_df, 11, 7, 7, batch_size, audio_features_id = audio_features_id, num_audio_features=num_audio_features, followers_id= followers_id)\n",
    "\n",
    "    # Train the model with the best chosen hyperparameter set\n",
    "    _,model = train_model(hidden_units, epochs, train_dataset)\n",
    "    evaluation_results = model.evaluate(test_dataset, verbose=0)\n",
    "    \n",
    "    # Obtain the R2 score for the model\n",
    "    predictions = model.predict(test_dataset, verbose=0)\n",
    "    prediction_vector = predictions.squeeze().flatten()\n",
    "    prediction_batched = np.expand_dims(prediction_vector, axis = 1)\n",
    "    \n",
    "    mse = evaluation_results[0]\n",
    "    mae = evaluation_results[1]\n",
    "    r2 = R2_Score(prediction_batched, targets_batched)\n",
    "\n",
    "    print(f\"When evaluated on testing set, the model {id+1} achieves MSE of {mse:.3f}, MAE of {mae:.3f} and R2 score of {r2:.3f}\")\n",
    "    \n",
    "    #Finding the loss between the model or baseline 2 predictions to the actual targets\n",
    "    loss_model = np.absolute(prediction_vector - targets)\n",
    "    loss_baseline = np.absolute(prediction_vector_2 - targets)\n",
    "    \n",
    "    #Running a t-test for each model loss against baseline 2 loss\n",
    "    t_statistic, p_value = scipy.stats.ttest_rel(loss_model, loss_baseline, alternative = \"less\")\n",
    "    \n",
    "    print(f\"Testing model {id+1} loss to baseline 2 loss achieves the statistics below:\")\n",
    "    print(\"t-Statistic:\", t_statistic)\n",
    "    print(\"p-Value:\", p_value)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
